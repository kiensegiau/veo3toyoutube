const fetch = require('node-fetch');
const fs = require('fs');
const path = require('path');
const FormData = require('form-data');

// ChatGPT API configuration
const OPENAI_API_KEY = process.env.OPENAI_API_KEY || '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';

// H√†m ph√¢n t√≠ch t·ª´ng gi√¢y chi ti·∫øt
async function analyzeSecondBySecond(frames, transcript, videoInfo) {
    try {
        console.log(`üîç [analyzeSecondBySecond] Ph√¢n t√≠ch chi ti·∫øt ${frames.length} frames (t·ª´ng gi√¢y)`);
        
        const detailedAnalysis = [];
        
        for (let i = 0; i < frames.length; i++) {
            const frame = frames[i];
            const second = i + 1;
            
            console.log(`üîç [analyzeSecondBySecond] Ph√¢n t√≠ch gi√¢y ${second}/${frames.length}`);
            
            // Chu·∫©n b·ªã image cho ChatGPT Vision
            let imageContent = null;
            if (fs.existsSync(frame.path)) {
                const imageBuffer = fs.readFileSync(frame.path);
                const base64Image = imageBuffer.toString('base64');
                imageContent = {
                    type: "image_url",
                    image_url: {
                        url: `data:image/jpeg;base64,${base64Image}`,
                        detail: "high"
                    }
                };
            }
            
            // T·∫°o prompt chi ti·∫øt cho t·ª´ng gi√¢y
            const systemPrompt = `B·∫°n l√† chuy√™n gia ph√¢n t√≠ch video frame-by-frame v·ªõi kh·∫£ nƒÉng m√¥ t·∫£ c·ª±c k·ª≥ chi ti·∫øt.
            
            Nhi·ªám v·ª•: Ph√¢n t√≠ch frame n√†y (gi√¢y ${second}) v√† m√¥ t·∫£ CHI TI·∫æT t·ª´ng element, t·ª´ng chi ti·∫øt nh·ªè nh·∫•t.
            
            Y√™u c·∫ßu m√¥ t·∫£ chi ti·∫øt:
            1. M√¥ t·∫£ t·ª´ng object, m√†u s·∫Øc, texture, √°nh s√°ng
            2. Ph√¢n t√≠ch composition, perspective, depth
            3. M√¥ t·∫£ camera angle, movement, focus
            4. Ph√¢n t√≠ch mood, atmosphere, emotion
            5. T·∫°o prompt Veo3 c·ª±c k·ª≥ chi ti·∫øt v√† c·ª• th·ªÉ
            
            Tr·∫£ v·ªÅ JSON format:
            {
                "second": ${second},
                "description": "m√¥ t·∫£ CHI TI·∫æT t·ª´ng element trong frame",
                "visual_elements": ["element1 v·ªõi m√¥ t·∫£ chi ti·∫øt", "element2 v·ªõi m√¥ t·∫£ chi ti·∫øt"],
                "colors": ["#color1 v·ªõi m√¥ t·∫£", "#color2 v·ªõi m√¥ t·∫£"],
                "textures": ["texture1", "texture2"],
                "lighting": "m√¥ t·∫£ chi ti·∫øt √°nh s√°ng",
                "shadows": "m√¥ t·∫£ chi ti·∫øt b√≥ng ƒë·ªï",
                "composition": "m√¥ t·∫£ chi ti·∫øt composition",
                "perspective": "m√¥ t·∫£ chi ti·∫øt perspective",
                "depth": "m√¥ t·∫£ chi ti·∫øt depth",
                "camera_angle": "m√¥ t·∫£ chi ti·∫øt g√≥c m√°y",
                "camera_movement": "m√¥ t·∫£ chi ti·∫øt chuy·ªÉn ƒë·ªông m√°y",
                "focus": "m√¥ t·∫£ chi ti·∫øt focus",
                "mood": "m√¥ t·∫£ chi ti·∫øt mood v√† atmosphere",
                "emotion": "m√¥ t·∫£ chi ti·∫øt c·∫£m x√∫c",
                "atmosphere": "m√¥ t·∫£ chi ti·∫øt kh√¥ng kh√≠",
                "veo3_prompt": "PROMPT C·ª∞C K·ª≤ CHI TI·∫æT cho Veo3 v·ªõi t·ª´ng element c·ª• th·ªÉ",
                "continuity": "m√¥ t·∫£ chi ti·∫øt c√°ch k·∫øt n·ªëi v·ªõi gi√¢y tr∆∞·ªõc/sau"
            }`;

            const userPrompt = `Ph√¢n t√≠ch CHI TI·∫æT frame n√†y (gi√¢y ${second}):
            
            Context: Video ${videoInfo.duration}s, ${videoInfo.width}x${videoInfo.height}
            Transcript: ${transcript}
            
            H√£y m√¥ t·∫£ CHI TI·∫æT t·ª´ng element trong frame:
            1. M√¥ t·∫£ t·ª´ng object, m√†u s·∫Øc c·ª• th·ªÉ, texture, material
            2. Ph√¢n t√≠ch √°nh s√°ng: h∆∞·ªõng, c∆∞·ªùng ƒë·ªô, m√†u s·∫Øc, b√≥ng ƒë·ªï
            3. M√¥ t·∫£ composition: v·ªã tr√≠ objects, balance, symmetry
            4. Ph√¢n t√≠ch perspective: g√≥c nh√¨n, depth, scale
            5. M√¥ t·∫£ camera: angle, movement, focus, depth of field
            6. Ph√¢n t√≠ch mood: atmosphere, emotion, feeling
            7. T·∫°o prompt Veo3 C·ª∞C K·ª≤ CHI TI·∫æT v·ªõi t·ª´ng element c·ª• th·ªÉ
            
            Prompt Veo3 ph·∫£i:
            - M√¥ t·∫£ t·ª´ng object c·ª• th·ªÉ v·ªõi m√†u s·∫Øc, texture, size
            - M√¥ t·∫£ √°nh s√°ng chi ti·∫øt: h∆∞·ªõng, c∆∞·ªùng ƒë·ªô, m√†u s·∫Øc
            - M√¥ t·∫£ camera movement c·ª• th·ªÉ: pan, zoom, tilt, dolly
            - M√¥ t·∫£ composition chi ti·∫øt: framing, rule of thirds
            - M√¥ t·∫£ mood v√† atmosphere c·ª• th·ªÉ
            - T·∫°o video 8 gi√¢y th√∫ v·ªã v√† li·ªÅn m·∫°ch`;

            // G·ªçi ChatGPT Vision API
            const response = await fetch('https://api.openai.com/v1/chat/completions', {
                method: 'POST',
                headers: {
                    'Authorization': `Bearer ${OPENAI_API_KEY}`,
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify({
                    model: "gpt-4o",
                    messages: [
                        {
                            role: "system",
                            content: systemPrompt
                        },
                        {
                            role: "user",
                            content: [
                                {
                                    type: "text",
                                    text: userPrompt
                                },
                                ...(imageContent ? [imageContent] : [])
                            ]
                        }
                    ],
                    max_tokens: 1000,
                    temperature: 0.7
                })
            });

            if (!response.ok) {
                const errorText = await response.text();
                throw new Error(`ChatGPT API error: ${response.status} - ${errorText}`);
            }

            const data = await response.json();
            const analysisText = data.choices[0].message.content;
            
            // Parse JSON response
            try {
                const jsonMatch = analysisText.match(/\{[\s\S]*\}/);
                if (jsonMatch) {
                    const analysis = JSON.parse(jsonMatch[0]);
                    detailedAnalysis.push({
                        second: second,
                        frame: frame,
                        analysis: analysis,
                        rawResponse: analysisText
                    });
                    console.log(`‚úÖ [analyzeSecondBySecond] Gi√¢y ${second} ph√¢n t√≠ch th√†nh c√¥ng`);
                } else {
                    throw new Error('No JSON found in response');
                }
            } catch (parseError) {
                console.warn(`‚ö†Ô∏è [analyzeSecondBySecond] Kh√¥ng th·ªÉ parse JSON cho gi√¢y ${second}, t·∫°o mock analysis`);
                
                // T·∫°o mock analysis cho gi√¢y n√†y
                const mockAnalysis = {
                    second: second,
                    description: `Frame at second ${second}`,
                    visual_elements: ["visual element 1", "visual element 2"],
                    mood: "professional",
                    camera_movement: "static",
                    lighting: "bright",
                    colors: ["#ffffff", "#000000"],
                    composition: "medium-shot",
                    veo3_prompt: `Create a professional video scene for second ${second}, with clean composition and bright lighting, showing technology and AI concepts`,
                    continuity: `Connects to previous seconds and leads to next seconds`
                };
                
                detailedAnalysis.push({
                    second: second,
                    frame: frame,
                    analysis: mockAnalysis,
                    rawResponse: analysisText
                });
            }
            
            // Delay ƒë·ªÉ tr√°nh rate limit
            if (i < frames.length - 1) {
                await new Promise(resolve => setTimeout(resolve, 1000));
            }
        }
        
        return {
            success: true,
            detailedAnalysis,
            totalSeconds: frames.length
        };
        
    } catch (error) {
        console.error('‚ùå [analyzeSecondBySecond] L·ªói:', error);
        return {
            success: false,
            error: error.message
        };
    }
}

// API endpoint
async function analyzeSecondBySecondAPI(req, res) {
    try {
        const { videoPath, startSecond = 0, duration = 8 } = req.body;
        
        if (!videoPath) {
            return res.status(400).json({
                success: false,
                message: 'Thi·∫øu videoPath'
            });
        }
        
        // Ki·ªÉm tra file t·ªìn t·∫°i
        if (!fs.existsSync(videoPath)) {
            return res.status(404).json({
                success: false,
                message: 'Video file kh√¥ng t·ªìn t·∫°i'
            });
        }
        
        console.log(`üîç [analyzeSecondBySecondAPI] Ph√¢n t√≠ch t·ª´ gi√¢y ${startSecond} trong ${duration} gi√¢y`);
        
        // Extract frames cho duration gi√¢y
        const { extractFramesFromVideo, getVideoInfo } = require('./extract-frames');
        
        const videoInfo = await getVideoInfo(videoPath);
        if (!videoInfo.success) {
            return res.status(500).json({
                success: false,
                message: 'Kh√¥ng th·ªÉ l·∫•y th√¥ng tin video',
                error: videoInfo.error
            });
        }
        
        // Extract frames cho duration gi√¢y
        const framesResult = await extractFramesFromVideo(videoPath, {
            count: duration,
            interval: 1, // M·ªói 1 gi√¢y 1 frame
            startTime: startSecond
        });
        
        if (!framesResult.success) {
            return res.status(500).json({
                success: false,
                message: 'Kh√¥ng th·ªÉ extract frames',
                error: framesResult.error
            });
        }
        
        // T·∫°o transcript
        const { generateMockTranscript } = require('./analyze-video-chatgpt');
        const transcript = await generateMockTranscript(videoInfo);
        
        // Ph√¢n t√≠ch chi ti·∫øt t·ª´ng gi√¢y
        const analysis = await analyzeSecondBySecond(framesResult.frames, transcript, videoInfo);
        
        if (!analysis.success) {
            return res.status(500).json({
                success: false,
                message: 'Kh√¥ng th·ªÉ ph√¢n t√≠ch chi ti·∫øt',
                error: analysis.error
            });
        }
        
        return res.json({
            success: true,
            message: `ƒê√£ ph√¢n t√≠ch chi ti·∫øt ${analysis.totalSeconds} gi√¢y`,
            videoInfo,
            frames: framesResult.frames,
            transcript,
            detailedAnalysis: analysis.detailedAnalysis
        });
        
    } catch (error) {
        console.error('‚ùå [analyzeSecondBySecondAPI] L·ªói:', error);
        return res.status(500).json({
            success: false,
            message: 'L·ªói ph√¢n t√≠ch chi ti·∫øt',
            error: error.message
        });
    }
}

module.exports = {
    analyzeSecondBySecond,
    analyzeSecondBySecondAPI
};
